{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEX\n",
    "\n",
    "-  [MODELING, loadING, EVOLVING](#modeling-loading-evolving)\n",
    "\n",
    "    1. [Modeling](#modeling)\n",
    "\n",
    "    2. [loading](#loading)\n",
    "\n",
    "    3. [Evolving](#evolving)\n",
    "\n",
    "- [QUERYING](#querying)\n",
    "    \n",
    "    1. [Query 1](#query-1:-find-the-top-3-most-cited-papers-of-each-conference.)\n",
    "\n",
    "    2. [Query 2](#query-2:-for-each-conference-find-its-community:-i.e.,-those-authors-that-have-published-papers-on-that-conference-in,-at-least,-4-different-editions.)\n",
    "\n",
    "    3. [Query 3](#query-3:-find-the-impact-factors-of-the-journals-in-your-graph.)\n",
    "\n",
    "    4. [Query 4](#query-4:-find-the-h-indexes-of-the-authors-in-your-graph.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"gwaB$3DMlab2\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING, loadING, EVOLVING\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(168)\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "num_authors = 100\n",
    "num_papers = 200\n",
    "num_conferences = 5\n",
    "num_journals = 3\n",
    "num_topics = 10\n",
    "num_keywords = 50\n",
    "num_reviews_per_paper = 3\n",
    "num_venues = 20\n",
    "num_years = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Authors\n",
    "authors = [\n",
    "    {\n",
    "        \"name\": faker.name(),\n",
    "        \"affiliation\": faker.company()\n",
    "    }\n",
    "    for _ in range(num_authors)\n",
    "]\n",
    "\n",
    "# Generate Topics and Keywords\n",
    "topics = [{\"topic\": topic} for topic in [\n",
    "    \"Machine Learning\",\n",
    "    \"Natural Language Processing\",\n",
    "    \"Computer Vision\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Robotics\",\n",
    "    \"Data Mining\",\n",
    "    \"Database Management\",\n",
    "    \"Neural Networks\",\n",
    "    \"Time-Series Analysis\",\n",
    "    \"Software Engineering\"\n",
    "]]\n",
    "\n",
    "keywords = []\n",
    "for topic in topics:\n",
    "    for _ in range(5):\n",
    "        keywords.append({\"word\": faker.word(), \"topic\": topic[\"topic\"]})\n",
    "\n",
    "# Generate Venues\n",
    "cities = [\n",
    "    \"New York\",\n",
    "    \"London\",\n",
    "    \"Tokyo\",\n",
    "    \"Paris\",\n",
    "    \"Sydney\",\n",
    "    \"Toronto\",\n",
    "    \"Berlin\",\n",
    "    \"Singapore\",\n",
    "    \"Hong Kong\",\n",
    "    \"Dubai\",\n",
    "    \"Los Angeles\",\n",
    "    \"Barcelona\",\n",
    "    \"Moscow\",\n",
    "    \"SÃ£o Paulo\",\n",
    "    \"Johannesburg\",\n",
    "    \"Rome\",\n",
    "    \"Istanbul\",\n",
    "    \"Beijing\",\n",
    "    \"Mumbai\",\n",
    "    \"Buenos Aires\"\n",
    "]\n",
    "\n",
    "venues = [{\"city\": city} for city in cities]\n",
    "\n",
    "# Generate Years\n",
    "years = [{\"year\": str(year)} for year in range(2024 - num_years, 2024)]\n",
    "\n",
    "# Generate Papers\n",
    "papers = []\n",
    "for _ in range(num_papers):\n",
    "    num_authors_per_paper = random.randint(2, 5)\n",
    "    list_of_authors_names = [author[\"name\"] for author in authors]\n",
    "    author_list = random.sample(list_of_authors_names  , k = num_authors_per_paper)\n",
    "    corresponding_author = random.choice(author_list)\n",
    "    author_list.remove(corresponding_author)\n",
    "    num_keywords_per_paper = random.randint(3, 7)\n",
    "    keyword_list = random.sample(keywords, k=num_keywords_per_paper)\n",
    "    \n",
    "    list_of_years = [year[\"year\"] for year in years]\n",
    "    year_paper = random.choice(list_of_years)\n",
    "    \n",
    "    prefix = \"https://doi.org/\"\n",
    "    directory = random.randint(10, 99)\n",
    "    journal_number = random.randint(1000000, 9999999)\n",
    "    article_number = random.randint(1000000, 9999999)\n",
    "    \n",
    "    \n",
    "    \n",
    "    paper = {\n",
    "        \"title\": faker.sentence(nb_words=6),\n",
    "        \"abstract\": faker.paragraph(nb_sentences=5),\n",
    "        \"year\": year_paper,\n",
    "        \"pages\": random.randint(5, 35),\n",
    "        \"DOI\": f\"{prefix}{directory}.{journal_number}.{year_paper}.{article_number}\",\n",
    "        \"authors\": author_list,\n",
    "        \"corresponding_author\": corresponding_author,\n",
    "        \"keywords\": keyword_list\n",
    "    }\n",
    "    papers.append(paper)\n",
    "\n",
    "# Generate Conferences and Editions\n",
    "conferences = [{\"name\": f\"Conference of {faker.word()}\"} for i in range(num_conferences)]\n",
    "\n",
    "editions = []\n",
    "for conf in conferences:\n",
    "    num_editions = random.randint(1, 20)\n",
    "    starting_year = random.randint(1970, 1970 + num_years - num_editions)\n",
    "    for i in range(num_editions):\n",
    "        edition = {\n",
    "            \"conference_name\": conf[\"name\"],\n",
    "            \"attendees\": random.randint(100, 1000),\n",
    "            \"venue\": random.choice(venues)[\"city\"],\n",
    "            \"year\": str(starting_year + i)\n",
    "        }\n",
    "        editions.append(edition)\n",
    "\n",
    "# Generate Journals and Volumes\n",
    "journals = [{\"name\": f\"Journal {i+1}\"} for i in range(num_journals)]\n",
    "\n",
    "volumes = []\n",
    "for journal in journals:\n",
    "    num_volumes = random.randint(1, 20)\n",
    "    for i in range(num_volumes):\n",
    "        volume = {\n",
    "            \"volume_number\": i + 1,\n",
    "            \"pages\": random.randint(50, 300)\n",
    "        }\n",
    "        volumes.append(volume)\n",
    "\n",
    "# Generate Reviews as Relationships\n",
    "reviews = []\n",
    "for paper in papers:\n",
    "    eligible_reviewers = [author for author in authors if author not in paper[\"authors\"]]\n",
    "    reviewers = random.sample(eligible_reviewers, k=num_reviews_per_paper)\n",
    "    for reviewer in reviewers:\n",
    "        review = {\n",
    "            \"paper_DOI\": paper[\"DOI\"],\n",
    "            \"reviewer_name\": reviewer[\"name\"]\n",
    "        }\n",
    "        reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faked data generated and saved to faked_data.json\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"authors\": authors,\n",
    "    \"papers\": papers,\n",
    "    \"conferences\": conferences,\n",
    "    \"editions\": editions,\n",
    "    \"journals\": journals,\n",
    "    \"volumes\": volumes,\n",
    "    \"keywords\": keywords,\n",
    "    \"topics\": topics,\n",
    "    \"reviews\": reviews,\n",
    "    \"venues\": venues,\n",
    "    \"years\": years\n",
    "}\n",
    "\n",
    "# Save to json file for safekeeping\n",
    "with open(\"faked_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(\"Faked data generated and saved to faked_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" we convert the json file to csv \"\"\"\n",
    "\n",
    "pathname_import = \"/Users/guillemmirabentrubinat/Library/Application Support/Neo4j Desktop/Application/relate-data/dbmss/dbms-85682a6c-ffc5-4848-8ae3-81bf5af53bcf/import\"\n",
    "\n",
    "# Save authors\n",
    "pd.DataFrame(data['authors']).to_csv(f'{pathname_import}/authors.csv', index=False)\n",
    "\n",
    "# Save topics\n",
    "pd.DataFrame(data['topics']).to_csv(f'{pathname_import}/topics.csv', index=False)\n",
    "\n",
    "# Save keywords\n",
    "keywords_df = pd.DataFrame(data['keywords'])\n",
    "keywords_df.to_csv(f'{pathname_import}/keywords.csv', index=False)\n",
    "\n",
    "# Save venues\n",
    "pd.DataFrame(data['venues']).to_csv(f'{pathname_import}/venues.csv', index=False)\n",
    "\n",
    "# Save years\n",
    "pd.DataFrame(data['years']).to_csv(f'{pathname_import}/years.csv', index=False)\n",
    "\n",
    "# Save papers\n",
    "papers = pd.DataFrame(data['papers'])\n",
    "papers.to_csv(f'{pathname_import}/papers.csv', index=False)\n",
    "\n",
    "# Save paper authors\n",
    "paper_authors = []\n",
    "for paper in data['papers']:\n",
    "    for author in paper['authors']:\n",
    "        paper_authors.append({'DOI': paper['DOI'], 'author_name': author})\n",
    "pd.DataFrame(paper_authors).to_csv(f'{pathname_import}/paper_authors.csv', index=False)\n",
    "\n",
    "# Save corresponding authors\n",
    "paper_corresponding_authors = []\n",
    "for paper in data['papers']:\n",
    "    paper_corresponding_authors.append({'DOI': paper['DOI'], 'corresponding_author': paper['corresponding_author']})\n",
    "pd.DataFrame(paper_corresponding_authors).to_csv(f'{pathname_import}/paper_corresponding_authors.csv', index=False)\n",
    "\n",
    "# Save paper keywords\n",
    "paper_keywords = []\n",
    "for paper in data['papers']:\n",
    "    for keyword in paper['keywords']:\n",
    "        paper_keywords.append({'DOI': paper['DOI'], 'keyword': keyword['word']})\n",
    "pd.DataFrame(paper_keywords).to_csv(f'{pathname_import}/paper_keywords.csv', index=False)\n",
    "\n",
    "# Save conferences\n",
    "pd.DataFrame(data['conferences']).to_csv(f'{pathname_import}/conferences.csv', index=False)\n",
    "\n",
    "# Save editions\n",
    "editions = pd.DataFrame(data['editions'])\n",
    "editions.to_csv(f'{pathname_import}/editions.csv', index=False)\n",
    "\n",
    "# Save journals\n",
    "pd.DataFrame(data['journals']).to_csv(f'{pathname_import}/journals.csv', index=False)\n",
    "\n",
    "# Save volumes\n",
    "volumes = pd.DataFrame(data['volumes'])\n",
    "volumes.to_csv(f'{pathname_import}/volumes.csv', index=False)\n",
    "\n",
    "# Save reviews\n",
    "reviews = pd.DataFrame(data['reviews'])\n",
    "reviews.to_csv(f'{pathname_import}/reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into Neo4j\n"
     ]
    }
   ],
   "source": [
    "def clear_database(tx):\n",
    "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "def load_data(tx):\n",
    "    # Load Authors\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///authors.csv' AS row\n",
    "    CREATE (:Author {name: row.name, affiliation: row.affiliation});\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Topics\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///topics.csv' AS row\n",
    "    CREATE (:Topic {topic: row.topic});\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Keywords\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///keywords.csv' AS row\n",
    "    MATCH (t:Topic {topic: row.topic})\n",
    "    CREATE (k:Keyword {word: row.word})-[:BELONGS_TO]->(t);\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Venues\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///venues.csv' AS row\n",
    "    CREATE (:Venue {city: row.city});\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Years\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///years.csv' AS row\n",
    "    CREATE (:Year {year: row.year});\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Papers\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///papers.csv' AS row\n",
    "    CREATE (p:Paper {title: row.title, abstract: row.abstract, year: row.year, pages: row.pages, DOI: row.DOI});\n",
    "    \"\"\")\n",
    "\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///paper_authors.csv' AS row\n",
    "    MATCH (p:Paper {DOI: row.DOI}), (a:Author {name: row.author_name})\n",
    "    CREATE (p)-[:AUTHORED_BY]->(a);\n",
    "    \"\"\")\n",
    "\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///paper_corresponding_authors.csv' AS row\n",
    "    MATCH (p:Paper {DOI: row.DOI}), (a:Author {name: row.corresponding_author})\n",
    "    CREATE (p)-[:CORRESPONDING_AUTHOR]->(a);\n",
    "    \"\"\")\n",
    "\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///paper_keywords.csv' AS row\n",
    "    MATCH (p:Paper {DOI: row.DOI}), (k:Keyword {word: row.keyword})\n",
    "    CREATE (p)-[:HAS_KEYWORD]->(k);\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Conferences\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///conferences.csv' AS row\n",
    "    CREATE (:Conference {name: row.name});\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Editions\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///editions.csv' AS row\n",
    "    MATCH (c:Conference {name: row.conference_name}), (v:Venue {city: row.venue}), (y:Year {year: row.year})\n",
    "    CREATE (e:Edition {conference_name: row.conference_name, attendees: row.attendees})-[:HELD_AT]->(v)-[:HELD_IN_YEAR]->(y)-[:PUBLISHED_IN_EDITION]->(c);\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Journals\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///journals.csv' AS row\n",
    "    CREATE (:Journal {name: row.name});\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Volumes\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///volumes.csv' AS row\n",
    "    MATCH (j:Journal {name: row.journal_name}), (y:Year {year: row.year})\n",
    "    CREATE (v:Volume {volume_number: row.volume_number, pages: row.pages})-[:PUBLISHED_IN_YEAR]->(y)-[:PUBLISHED_IN_JOURNAL]->(j);\n",
    "    \"\"\")\n",
    "\n",
    "    # Load Reviews\n",
    "    tx.run(\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///reviews.csv' AS row\n",
    "    MATCH (p:Paper {DOI: row.paper_DOI}), (a:Author {name: row.reviewer_name})\n",
    "    CREATE (p)-[:REVIEWED_BY]->(a);\n",
    "    \"\"\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Clear the database\n",
    "    session.execute_write(clear_database)\n",
    "\n",
    "    # Load the data\n",
    "    session.execute_write(load_data)\n",
    "\n",
    "print(\"Data loaded into Neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolving\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUERYING\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1: Find the top 3 most cited papers of each conference.\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2: For each conference find its community: i.e., those authors that have published papers on that conference in, at least, 4 different editions.\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3: Find the impact factors of the journals in your graph.\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4: Find the h-indexes of the authors in your graph.\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
